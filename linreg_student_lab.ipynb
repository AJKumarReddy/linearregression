{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression & GLMs — Student Lab\n",
        "\n",
        "Complete all TODOs. Avoid sklearn for core parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic Dataset (with collinearity)\n",
        "We generate data where features can be highly correlated to motivate ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n",
            "corr(x0,x1)= 0.9984746876366171\n"
          ]
        }
      ],
      "source": [
        "def make_regression(n=250, d=5, noise=0.5, collinear=True):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    if collinear and d >= 2:\n",
        "        X[:, 1] = X[:, 0] + 0.05 * rng.standard_normal(n)\n",
        "    w_true = rng.standard_normal(d)\n",
        "    y = X @ w_true + noise * rng.standard_normal(n)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_regression()\n",
        "n, d = X.shape\n",
        "check('shapes', y.shape == (n,))\n",
        "print('corr(x0,x1)=', np.corrcoef(X[:,0], X[:,1])[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — OLS Closed Form\n",
        "\n",
        "### Task 1.1: Closed-form w_hat using solve\n",
        "\n",
        "# TODO: compute w_hat using solve on (X^T X) w = X^T y\n",
        "# HINT: `XtX = X.T@X`, `Xty = X.T@y`, `np.linalg.solve(XtX, Xty)`\n",
        "\n",
        "**Checkpoint:** Why is explicit inverse discouraged?\n",
        "\n",
        "explicit inverse is more unstable and computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: w_shape\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "XtX = X.T @ X\n",
        "Xty = X.T @ y\n",
        "w_hat = np.linalg.solve(XtX, Xty) \n",
        " \n",
        "check('w_shape', w_hat.shape == (d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Evaluate fit + residuals\n",
        "Compute:\n",
        "- predictions y_pred\n",
        "- MSE\n",
        "- residual mean and std\n",
        "\n",
        "**Interview Angle:** What does a structured residual pattern imply (e.g., nonlinearity)?\n",
        "\n",
        "it imples current model is not exactly suitable. it failed to capture a pattern. we should try others models. for example, if the current model is a linear model. we should try polynomial model or see if all features are included in linear model or look for any corelation errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mse 0.260799382964824 resid_mean 0.022970458235497877 resid_std 0.5101683457578245\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "y_pred = np.dot(X, w_hat) # predicting y from X and w_hat : y_pred = X * w_hat\n",
        "mse = float(np.mean((y - y_pred) ** 2)) # mean squared error between y and y_pred\n",
        "resid = y_pred - y # residuals between y_pred and y\n",
        "\n",
        "print('mse', mse, 'resid_mean', resid.mean(), 'resid_std', resid.std())\n",
        "check('finite', np.isfinite(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Gradient Descent\n",
        "\n",
        "### Task 2.1: Implement MSE loss + gradient\n",
        "\n",
        "Loss = mean((Xw-y)^2), grad = (2/n) X^T(Xw-y)\n",
        "\n",
        "# TODO: implement `mse_loss_and_grad`\n",
        "\n",
        "**FAANG gotcha:** shapes and constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: grad_shape\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "def mse_loss_and_grad(X, y, w):\n",
        "    # TODO\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "    y = np.asarray(y, dtype=np.float64)\n",
        "    w = np.asarray(w, dtype=np.float64)\n",
        "    \n",
        "    r = X @ w - y\n",
        "    \"\"\"\n",
        "    in class we used (1/n) sum r_i^2 for loss function formula. Here I used (1/2n) sum r_i^2.\n",
        "    because many standard reference books were using this form. I hoope it's ok.\n",
        "    gradient formula is adusted accordingly.\n",
        "    \"\"\"\n",
        "    loss = 0.5 * np.mean(r ** 2) # mean squared error loss\n",
        "    grad = (X.T @ r) / X.shape[0] # gradient of loss\n",
        "    return loss, grad\n",
        "\n",
        "w0 = np.zeros(d)\n",
        "loss0, g0 = mse_loss_and_grad(X, y, w0)\n",
        "check('grad_shape', g0.shape == (d,))\n",
        "check('finite_loss', np.isfinite(loss0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Train with GD + compare to closed-form\n",
        "\n",
        "# TODO: implement a simple GD loop, track loss, and compare final weights to w_hat.\n",
        "\n",
        "**Checkpoint:** How does feature scaling affect GD?\n",
        "\n",
        "feature scaling changes the value of corrospding weights. when we adjust the weights with GD, using feature scaling makes GD stable and smooth by avoiding zig-zag pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final_loss 0.13150579862647271\n",
            "||w_gd-w_hat|| 1.3164915910450554\n",
            "OK: loss_decreases\n"
          ]
        }
      ],
      "source": [
        "def train_gd(X, y, lr=0.05, steps=500):\n",
        "    # TODO\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "    y = np.asarray(y, dtype=np.float64)\n",
        "    \n",
        "    w = np.zeros(X.shape[1]) # initializing weights to zero as starting point\n",
        "    losses = [] # to store loss values\n",
        "    for i in range(steps): #steps is number of iterations\n",
        "        loss, grad = mse_loss_and_grad(X, y, w) # computing loss and gradient\n",
        "        w = w - lr * grad # updating weights\n",
        "        losses.append(loss)\n",
        "    return w, losses\n",
        "\n",
        "w_gd, losses = train_gd(X, y, lr=0.05, steps=500)\n",
        "print('final_loss', losses[-1])\n",
        "print('||w_gd-w_hat||', np.linalg.norm(w_gd - w_hat))\n",
        "check('loss_decreases', losses[-1] <= losses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Ridge Regression (L2)\n",
        "\n",
        "### Task 3.1: Ridge closed-form\n",
        "w = (X^T X + λI)^{-1} X^T y\n",
        "\n",
        "# TODO: implement ridge_solve\n",
        "\n",
        "**Interview Angle:** Why does ridge help under collinearity?\n",
        "\n",
        "Ridge regression helps in recuding high varience by altering weights(by adding additional cost from weights). when the features are linearly dependent it shrinks them thus helping with collinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: ridge_shape\n"
          ]
        }
      ],
      "source": [
        "def ridge_solve(X, y, lam):\n",
        "    # TODO\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "    y = np.asarray(y, dtype=np.float64)\n",
        "    lam = lam if lam > 0 else 0.0 # ensure lambda is non-negative\n",
        "    d = X.shape[1] # number of features\n",
        "    return np.linalg.solve(X.T @ X + lam * np.eye(d), X.T @ y) # ridge regression formula, lambda = lam = 1.0 here.\n",
        " \n",
        "w_ridge = ridge_solve(X, y, lam=1.0)\n",
        "check('ridge_shape', w_ridge.shape == (d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Bias/variance demo with train/test split\n",
        "\n",
        "# TODO: split into train/test and compare MSE for multiple lambdas.\n",
        "\n",
        "**Checkpoint:** why can test error improve even when train error worsens?\n",
        "\n",
        "Because the model is generalizing (Not overfitting to training data) which makes the train error worse. since its generalizing the test data fits better, hence test error is lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lam, train_mse, test_mse\n",
            "(0.0, 0.2673763268904879, 0.2500584551029355)\n",
            "(0.1, 0.26761204346193085, 0.25016353854476864)\n",
            "(1.0, 0.26907830138935623, 0.25188423277193805)\n",
            "(10.0, 0.2755994368557424, 0.26284506240445366)\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "idx = rng.permutation(n)\n",
        "train = idx[: int(0.7*n)]\n",
        "test = idx[int(0.7*n):]\n",
        "Xtr, ytr = X[train], y[train]\n",
        "Xte, yte = X[test], y[test]\n",
        "\n",
        "lams = [0.0, 0.1, 1.0, 10.0]\n",
        "results = []\n",
        "for lam in lams:\n",
        "    w = ridge_solve(Xtr, ytr, lam=lam) if lam > 0 else np.linalg.solve(Xtr.T@Xtr, Xtr.T@ytr)\n",
        "    tr_mse = np.mean((Xtr@w - ytr)**2)\n",
        "    te_mse = np.mean((Xte@w - yte)**2)\n",
        "    results.append((lam, tr_mse, te_mse))\n",
        "print('lam, train_mse, test_mse')\n",
        "for r in results:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — GLM Intuition\n",
        "\n",
        "### Task 4.1: Match tasks to (distribution, link)\n",
        "Fill in a table for:\n",
        "- regression\n",
        "- binary classification\n",
        "- count prediction\n",
        "\n",
        "**Explain:** what changes when you go from OLS to a GLM?\n",
        "\n",
        "GLM can handle non - linear(like polynomial) and non - gaussian (like classification) realtions while OLS cannot be used for them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Problem | Target type | Distribution | Link | Loss |\n",
        "|---|---|---|---|---|\n",
        "| House price | continuous | Guassian | identity | MSE |\n",
        "| Fraud | binary | Bernouli | logit | BCE / log loss |\n",
        "| Clicks per user | count | poisson | log | poisson devience |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Train/test results shown for ridge\n",
        "- Short answers to checkpoint questions\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
